import type { LLMStreamKind } from '@/lib/llm-observe/types'
import type { InternalLLMStreamStepMeta } from '@/lib/llm-observe/internal-stream-context'

export interface ChatCompletionOptions {
    temperature?: number
    reasoning?: boolean
    reasoningEffort?: 'minimal' | 'low' | 'medium' | 'high'
    maxRetries?: number
    // ðŸ’° è®¡è´¹ç›¸å…³
    projectId?: string   // ç”¨äºŽè®¡è´¹ï¼ˆå¦‚æžœä¸ä¼ ï¼Œä½¿ç”¨ 'system' ä½œä¸ºé»˜è®¤å€¼ï¼‰
    action?: string      // è®¡è´¹æ“ä½œåç§°
    // æµå¼æ­¥éª¤å…ƒä¿¡æ¯ï¼ˆç”¨äºŽä»»åŠ¡æŽ§åˆ¶å°æŒ‰æ­¥éª¤å±•ç¤ºï¼‰
    streamStepId?: string
    streamStepAttempt?: number
    streamStepTitle?: string
    streamStepIndex?: number
    streamStepTotal?: number
    // å†…éƒ¨ä¿æŠ¤ä½ï¼šé¿å… chatCompletion ä¸Ž chatCompletionStream äº’ç›¸é€’å½’
    __skipAutoStream?: boolean
}

export interface ChatCompletionStreamCallbacks {
    onStage?: (stage: {
        stage: 'submit' | 'streaming' | 'fallback' | 'completed'
        provider?: string | null
        step?: InternalLLMStreamStepMeta
    }) => void
    onChunk?: (chunk: {
        kind: LLMStreamKind
        delta: string
        seq: number
        lane?: string | null
        step?: InternalLLMStreamStepMeta
    }) => void
    onComplete?: (text: string, step?: InternalLLMStreamStepMeta) => void
    onError?: (error: unknown, step?: InternalLLMStreamStepMeta) => void
}

export type ChatMessage = { role: 'user' | 'assistant' | 'system'; content: string }
